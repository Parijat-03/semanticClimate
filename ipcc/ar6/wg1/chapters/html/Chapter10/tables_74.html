<div><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Cross-Chapter Box 10.3 (continued)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The assessment is derived following the IPCC uncertainty guidance through a distillation process of multiple lines of evidence on\nobserved trends, attribution of trends or events, climate model projections and physical understanding, covered in several chapters\nof the WGI Report.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>In particular, this Cross-Chapter Box explains the methodology used to derive the regional assessments summarized in the Technical\nSummary (TS) table that are, in turn, used as a basis for the synthesis assessment in the Summary for Policymakers (SPM).</td>
    </tr>
    <tr>
      <th>3</th>
      <td>The process consists of three discrete steps, listed below and schematically illustrated in Cross-Chapter Box 10.3, Figure 1:</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1. Collection and assessment of the fitness-for-purpose of available information</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Any specific climate change that is regionally relevant is assessed looking at lines of evidence, potentially across multiple indices. For</td>
    </tr>
    <tr>
      <th>6</th>
      <td>example, several definitions of &#8216;drought&#8217; exist that refer to a variety of the underlying processes, temporal and spatial scales, as well as\nsectoral applications and associated impacts (Sections 11.6 and 12.3). Such diverse definitions need to be gathered from the relevant\nliterature, compared, and individually assessed if appropriate.</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Once the indices of change are properly defined, the relevant climate information is collated from the available sources.</td>
    </tr>
    <tr>
      <th>8</th>
      <td>The information is then evaluated against its fitness-for-purpose, for example, whether it is adequate to provide robust evidence to\nderive an assessment. In the case of observed data, issues to be considered include (but are not limited to): spatial and temporal\nresolution, accuracy, gaps in the recorded data, homogeneity in the station network, uncertainty treatment, etc. (Sections 10.2, 11.2,\n11.9, 12.4; Atlas.1.4). In the case of modelled data, an assessment of the fitness-for-purpose typically includes an evaluation of\nnumerical or statistical methods adopted, adequate representation of the physical processes, forcings and feedbacks relevant for the\nregion and the change under consideration, the availability of adequate ensembles to assess the interplay between forced response\nand internal variability and the uncertainty in future projections (Sections 10.3, 10.4, 11.2, 11.9, 12.4 and Chapter Atlas). Attribution\nassessments are usually based on models and observations for which the fitness-for-purpose is assessed with similar criteria as\nthose described above (Cross-Working Group Box: Attribution in Chapter 1). The assessment is made either directly or indirectly by\nscrutinizing the data and methods of the relevant literature against the criteria listed above.</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2. Assessment of confidence of the multiple lines of evidence</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Once the relevant information has been collated for a given regional change, an assessment of the confidence is first made for each</td>
    </tr>
    <tr>
      <th>11</th>
      <td>line of evidence separately. The assessment of confidence is the result of expert judgment drawing around a set of questions such as:</td>
    </tr>
    <tr>
      <th>12</th>
      <td></td>
    </tr>
    <tr>
      <th>13</th>
      <td>&#8226; Do we have a physical explanation of the processes responsible for past and future changes in the region?</td>
    </tr>
    <tr>
      <th>14</th>
      <td>&#8226; Do observed trends agree amongst different observational products/datasets? Are they statistically significant? Do the observations</td>
    </tr>
    <tr>
      <th>15</th>
      <td>cover the same temporal period and/or spatial area? Are the observations homogeneous in time?</td>
    </tr>
    <tr>
      <th>16</th>
      <td>&#8226; Can past trends be attributed to human activities (greenhouse gases, short-lived climate forcers or land-use/management</td>
    </tr>
    <tr>
      <th>17</th>
      <td>changes)? Are attributed trends and events consistent? What is the interplay between internal variability and forced response?</td>
    </tr>
    <tr>
      <th>18</th>
      <td>&#8226; Do model projections agree on the magnitude and sign of the projected signal? Are we able to understand the reasons underlying</td>
    </tr>
    <tr>
      <th>19</th>
      <td>any discrepancies? Can we quantify the uncertainty in the projected signal? Are the projections based on similar SSP-RCP/time\nhorizon or global warming level (GWL; Cross-Chapter Box 11.1)? If not, are they comparable?</td>
    </tr>
    <tr>
      <th>20</th>
      <td>&#8226; Has the signal already emerged? Are there studies indicating the time of emergence of the signal?</td>
    </tr>
    <tr>
      <th>21</th>
      <td></td>
    </tr>
    <tr>
      <th>22</th>
      <td>The assessment is then tested for overall coherence across the available lines of evidence, for example:</td>
    </tr>
    <tr>
      <th>23</th>
      <td></td>
    </tr>
    <tr>
      <th>24</th>
      <td>&#8226; Are observed historical changes consistent with future projections?</td>
    </tr>
    <tr>
      <th>25</th>
      <td>&#8226; Are attributed events similar to the types of changes projected for the future?</td>
    </tr>
    <tr>
      <th>26</th>
      <td></td>
    </tr>
    <tr>
      <th>27</th>
      <td>&#8226; Is there a physical explanation for changes that are projected but have not yet been clearly observed or attributed?</td>
    </tr>
    <tr>
      <th>28</th>
      <td></td>
    </tr>
    <tr>
      <th>29</th>
      <td>&#8226; Are assessments of confidence and likelihood performed in a similar way across regions?</td>
    </tr>
    <tr>
      <th>30</th>
      <td></td>
    </tr>
    <tr>
      <th>31</th>
      <td>3. Distillation of regional information and synthesis of the independent assessments</td>
    </tr>
    <tr>
      <th>32</th>
      <td>To ensure transparency, a traceback matrix is constructed (refer to 10.SM) that, for each region and index, identifies where in the</td>
    </tr>
    <tr>
      <th>33</th>
      <td>chapters the relevant information can be found, together with a summary of the relevant information in the Technical Summary.</td>
    </tr>
  </tbody>
</table></div>